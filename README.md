# Local AI Applications with Haystack

## Tools

This project is built using an open-source stack for local LLM development:

![Local LLMs via LM Studio](https://img.shields.io/badge/Local_LLMs-LM_Studio-7B3FE4?logo=lmstudio&logoColor=white&style=flat-square)
![RAG Pipeline by Haystack](https://img.shields.io/badge/RAG_Pipeline-Haystack-FBCB0A?logo=haystack&logoColor=black&style=flat-square)
![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg?style=flat-square&logo=python&logoColor=white)

## Acknowledgements

- This project is based on the *Local AI Applications with Haystack* course by **DeepLearning.AI**, and the objective of this repository is solely to
    explore its model agnostic capabilities.
- This project uses the [Haystackâ€¯2](https://github.com/deepset-ai/haystack) AI openâ€‘source framework.


Handsâ€‘on notebooks that show how to build  Retrievalâ€‘Augmented Generation (RAG) and agent pipelines with [HaystackÂ 2](https://docs.haystack.deepset.ai/).\
All models run on your own hardwareâ€”no API keys required. Some other API services with free trial keys are used, but are optional.

## List of content

| Folder                               | Highlights                                               |
| ------------------------------------ | -------------------------------------------------------- |
| **1â€‘Basic Components in Haystack**   | Minimal example: `DocumentStore â†’ Retriever â†’ Generator` |
| **2â€‘CustomizedÂ RAG**                 | Prompt engineering, rerankers, hybrid retrieval          |
| **3â€‘Creating Custom Components**     | Write & register your own Haystack Component             |
| **4â€‘Branching Pipelines**            | Conditional routers, parallel document stores            |
| **5â€‘Selfâ€‘ReflectingÂ Agent Pipeline** | Autonomous agent that iteratively refines its answers    |

How to run the

##  How to run this notebook

First, you will need to have VS code installed in your WSL 2.

Then, you will need to have installed LM studio on your computer, load an LLM model and start a server.
You will need to copy the server URL.

In your WSL : 

```bash

# 1. Clone & enter
git clone https://github.com/davalpez/Local-AI-aplications-with-Haystack.git
cd Local-AI-aplications-with-Haystack

# 2. Create an isolated environment (Python â‰¥â€¯3.10)
python -m venv .venv
source .venv/bin/activate  

# 3. Install requirements
pip install -r requirements.txt \

```


## ðŸ“œ License

This project is licensed under the **MIT License** â€” see the [LICENSE](LICENSE) file for details.


